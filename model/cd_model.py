# import logging
# from collections import OrderedDict

# import torch
# import torch.nn as nn
# import os
# import model.networks as networks
# from .base_model import BaseModel
# from misc.metric_tools import ConfuseMatrixMeter
# from misc.torchutils import get_scheduler
# logger = logging.getLogger('base')


# class CD(BaseModel):
#     def __init__(self, opt):
#         super(CD, self).__init__(opt)
#         # define network and load pretrained models
#         self.netCD = self.set_device(networks.define_CD(opt))

#         # set loss and load resume state
#         self.loss_type = opt['model_cd']['loss_type']
#         if self.loss_type == 'ce':
#             self.loss_func =nn.CrossEntropyLoss().to(self.device)
#         else:
#             raise NotImplementedError()
        
#         if self.opt['phase'] == 'train':
#             self.netCD.train()
#             # find the parameters to optimize
#             optim_cd_params = list(self.netCD.parameters())

#             if opt['train']["optimizer"]["type"] == "adam":
#                 self.optCD = torch.optim.Adam(
#                     optim_cd_params, lr=opt['train']["optimizer"]["lr"])
#             elif opt['train']["optimizer"]["type"] == "adamw":
#                 self.optCD = torch.optim.AdamW(
#                     optim_cd_params, lr=opt['train']["optimizer"]["lr"])
#             else:
#                 raise NotImplementedError(
#                     'Optimizer [{:s}] not implemented'.format(opt['train']["optimizer"]["type"]))
#             self.log_dict = OrderedDict()
            
#             #Define learning rate sheduler
#             self.exp_lr_scheduler_netCD = get_scheduler(optimizer=self.optCD, args=opt['train'])
#         else:
#             self.netCD.eval()
#             self.log_dict = OrderedDict()

#         self.load_network()
#         self.print_network()

#         self.running_metric = ConfuseMatrixMeter(n_class=opt['model_cd']['out_channels'])
#         self.len_train_dataloader = opt["len_train_dataloader"]
#         self.len_val_dataloader = opt["len_val_dataloader"]

#     # Feeding all data to the CD model
#     def feed_data(self, feats_A, feats_B, data):
#         self.feats_A = feats_A
#         self.feats_B = feats_B
#         self.data = self.set_device(data)

#     # Optimize the parameters of the CD model
#     def optimize_parameters(self):
#         self.optCD.zero_grad()
#         self.pred_cm = self.netCD(self.feats_A, self.feats_B)
#         l_cd = self.loss_func(self.pred_cm, self.data["L"].long())
#         l_cd.backward()
#         self.optCD.step()
#         self.log_dict['l_cd'] = l_cd.item()

#     # Testing on given data
#     def test(self):
#         self.netCD.eval()
#         with torch.no_grad():
#             if isinstance(self.netCD, nn.DataParallel):
#                 self.pred_cm = self.netCD.module.forward(self.feats_A, self.feats_B)
#             else:
#                 self.pred_cm = self.netCD(self.feats_A, self.feats_B)
#             l_cd = self.loss_func(self.pred_cm, self.data["L"].long())
#             self.log_dict['l_cd'] = l_cd.item()
#         self.netCD.train()

#     # Get current log
#     def get_current_log(self):
#         return self.log_dict

#     # Get current visuals
#     def get_current_visuals(self):
#         out_dict = OrderedDict()
#         out_dict['pred_cm'] = torch.argmax(self.pred_cm, dim=1, keepdim=False)
#         out_dict['gt_cm'] = self.data['L']
#         return out_dict

#     # Printing the CD network
#     def print_network(self):
#         s, n = self.get_network_description(self.netCD)
#         if isinstance(self.netCD, nn.DataParallel):
#             net_struc_str = '{} - {}'.format(self.netCD.__class__.__name__,
#                                              self.netCD.module.__class__.__name__)
#         else:
#             net_struc_str = '{}'.format(self.netCD.__class__.__name__)

#         logger.info(
#             'Change Detection Network structure: {}, with parameters: {:,d}'.format(net_struc_str, n))
#         logger.info(s)

#     # Saving the network parameters
#     def save_network(self, epoch, is_best_model = False):
#         cd_gen_path = os.path.join(
#             self.opt['path']['checkpoint'], 'cd_model_E{}_gen.pth'.format(epoch))
#         cd_opt_path = os.path.join(
#             self.opt['path']['checkpoint'], 'cd_model_E{}_opt.pth'.format(epoch))
        
#         if is_best_model:
#             best_cd_gen_path = os.path.join(
#                 self.opt['path']['checkpoint'], 'best_cd_model_gen.pth'.format(epoch))
#             best_cd_opt_path = os.path.join(
#                 self.opt['path']['checkpoint'], 'best_cd_model_opt.pth'.format(epoch))

#         # Save CD model pareamters
#         network = self.netCD
#         if isinstance(self.netCD, nn.DataParallel):
#             network = network.module
#         state_dict = network.state_dict()
#         for key, param in state_dict.items():
#             state_dict[key] = param.cpu()
#         torch.save(state_dict, cd_gen_path)
#         if is_best_model:
#             torch.save(state_dict, best_cd_gen_path)


#         # Save CD optimizer paramers
#         opt_state = {'epoch': epoch,
#                      'scheduler': None, 
#                      'optimizer': None}
#         opt_state['optimizer'] = self.optCD.state_dict()
#         torch.save(opt_state, cd_opt_path)
#         if is_best_model:
#             torch.save(opt_state, best_cd_opt_path)

#         # Print info
#         logger.info(
#             'Saved current CD model in [{:s}] ...'.format(cd_gen_path))
#         if is_best_model:
#             logger.info(
#             'Saved best CD model in [{:s}] ...'.format(best_cd_gen_path))

#     # Loading pre-trained CD network
#     def load_network(self):
#         load_path = self.opt['path_cd']['resume_state']
#         if load_path is not None:
#             logger.info(
#                 'Loading pretrained model for CD model [{:s}] ...'.format(load_path))
#             gen_path = '{}_gen.pth'.format(load_path)
#             opt_path = '{}_opt.pth'.format(load_path)
            
#             # change detection model
#             network = self.netCD
#             if isinstance(self.netCD, nn.DataParallel):
#                 network = network.module
#             network.load_state_dict(torch.load(
#                 gen_path), strict=True)
            
#             if self.opt['phase'] == 'train':
#                 opt = torch.load(opt_path)
#                 self.optCD.load_state_dict(opt['optimizer'])
#                 self.begin_step = opt['iter']
#                 self.begin_epoch = opt['epoch']
    
#     # Functions related to computing performance metrics for CD
#     def _update_metric(self):
#         """
#         update metric
#         """
#         G_pred = self.pred_cm.detach()
#         G_pred = torch.argmax(G_pred, dim=1)

#         current_score = self.running_metric.update_cm(pr=G_pred.cpu().numpy(), gt=self.data['L'].detach().cpu().numpy())
#         return current_score
    
#     # Collecting status of the current running batch
#     def _collect_running_batch_states(self):
#         self.running_acc = self._update_metric()
#         self.log_dict['running_acc'] = self.running_acc.item()

#     # Collect the status of the epoch
#     def _collect_epoch_states(self):
#         scores = self.running_metric.get_scores()
#         self.epoch_acc = scores['mf1']
#         self.log_dict['epoch_acc'] = self.epoch_acc.item()

#         for k, v in scores.items():
#             self.log_dict[k] = v
#             #message += '%s: %.5f ' % (k, v)

#     # Rest all the performance metrics
#     def _clear_cache(self):
#         self.running_metric.clear()

#     # Finctions related to learning rate sheduler
#     def _update_lr_schedulers(self):
#         self.exp_lr_scheduler_netCD.step()

        
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
import os
import model.networks as networks
from .base_model import BaseModel
from misc.metric_tools import ConfuseMatrixMeter
from misc.torchutils import get_scheduler

# å¯¼å…¥ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°
from .physical_loss import create_physics_loss

logger = logging.getLogger('base')

class DDPMCDModel(BaseModel):
    """
    DDPMå˜åŒ–æ£€æµ‹æ¨¡å‹ï¼Œé›†æˆç‰©ç†çº¦æŸæŸå¤±
    """
    def __init__(self, opt):
        super(DDPMCDModel, self).__init__(opt)
        
        # åˆå§‹åŒ–å¿…è¦çš„å±æ€§ï¼ˆBaseModelæ²¡æœ‰æä¾›ï¼‰
        self.optimizers = []
        self.schedulers = []
        self.is_train = opt['phase'] == 'train'
        
        # å®šä¹‰ç½‘ç»œ
        self.netCD = self.set_device(networks.define_CD(opt))

        # **é‡è¦ä¿®æ”¹ï¼šæ— è®ºè®­ç»ƒè¿˜æ˜¯æµ‹è¯•éƒ½éœ€è¦è®¾ç½®æŸå¤±å‡½æ•°**
        self.setup_loss_functions(opt)
        
        # è®¾ç½®è®­ç»ƒçŠ¶æ€
        if self.is_train:
            self.netCD.train()
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            train_opt = opt['train']
            optim_params = list(self.netCD.parameters())
            
            if train_opt['optimizer']['type'] == 'adam':
                self.optimizer = torch.optim.Adam(
                    optim_params, 
                    lr=train_opt['optimizer']['lr'],
                    weight_decay=train_opt['optimizer'].get('weight_decay', 0)
                )
            elif train_opt['optimizer']['type'] == 'adamw':
                self.optimizer = torch.optim.AdamW(
                    optim_params,
                    lr=train_opt['optimizer']['lr'], 
                    weight_decay=train_opt['optimizer'].get('weight_decay', 1e-2)
                )
            else:
                raise NotImplementedError(f"ä¼˜åŒ–å™¨ {train_opt['optimizer']['type']} æœªå®ç°")
            
            # ä¸ºäº†å…¼å®¹æ€§ï¼ŒåŒæ—¶ä¿å­˜ä¸ºoptCD
            self.optCD = self.optimizer
            self.optimizers.append(self.optimizer)
            
            # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
            self.setup_schedulers()
            
            # åˆ›å»ºæŸå¤±å‡½æ•°
            self.setup_loss_functions(opt)
            
            # è®­ç»ƒæŒ‡æ ‡
            self.log_dict = OrderedDict()
        else:
            # æµ‹è¯•æ¨¡å¼
            self.netCD.eval()
            self.log_dict = OrderedDict()
        
        # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ªå™¨
        self.running_metric = ConfuseMatrixMeter(n_class=opt['model_cd']['out_channels'])
        
        # æ•°æ®é›†é•¿åº¦ä¿¡æ¯
        if 'len_train_dataloader' in opt:
            self.len_train_dataloader = opt["len_train_dataloader"]
        if 'len_val_dataloader' in opt:
            self.len_val_dataloader = opt["len_val_dataloader"]
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.load_network()
        
        # æ‰“å°ç½‘ç»œä¿¡æ¯
        self.print_network()
        
    def setup_loss_functions(self, opt):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        loss_type = opt['model_cd'].get('loss_type', 'ce')
        
        if loss_type == 'physics_constrained':
            # ä½¿ç”¨ç‰©ç†çº¦æŸæŸå¤±
            physics_config = opt['model_cd'].get('physics_loss', {})
            self.criterion = create_physics_loss(physics_config)
            self.criterion = self.criterion.to(self.device)
            print("ğŸ”§ ä½¿ç”¨ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°")
            
        elif loss_type == 'ce':
            # æ ‡å‡†äº¤å‰ç†µæŸå¤±
            self.criterion = nn.CrossEntropyLoss().to(self.device)
            print("ğŸ”§ ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°")
            
        elif loss_type == 'bce':
            # äºŒå…ƒäº¤å‰ç†µæŸå¤±
            self.criterion = nn.BCEWithLogitsLoss().to(self.device)
            print("ğŸ”§ ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°")
            
        elif loss_type == 'focal':
            # FocalæŸå¤±
            self.criterion = self.focal_loss
            print("ğŸ”§ ä½¿ç”¨FocalæŸå¤±å‡½æ•°")
            
        else:
            raise NotImplementedError(f"æŸå¤±ç±»å‹ {loss_type} æœªå®ç°")
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼ŒåŒæ—¶ä¿å­˜ä¸ºloss_func
        self.loss_func = self.criterion
    
    def setup_schedulers(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.is_train and 'scheduler' in self.opt['train']:
            self.exp_lr_scheduler_netCD = get_scheduler(
                optimizer=self.optimizer, 
                args=self.opt['train']
            )
            if self.exp_lr_scheduler_netCD:
                self.schedulers.append(self.exp_lr_scheduler_netCD)
    
    def focal_loss(self, pred, target, alpha=1, gamma=2):
        """Focal Losså®ç°"""
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = alpha * (1-pt)**gamma * ce_loss
        return focal_loss.mean()
    
    # å…¼å®¹æ—§æ¥å£çš„feed_data
    def feed_data(self, *args):
        """è¾“å…¥æ•°æ® - å…¼å®¹æ–°æ—§æ¥å£"""
        if len(args) == 1:
            # æ–°æ¥å£ï¼šfeed_data(data)
            data = args[0]
            self.data = self.set_device(data)
            self.image_A = self.data.get('A')
            self.image_B = self.data.get('B')
            self.label = self.data.get('label', self.data.get('L'))  # å…¼å®¹'L'å’Œ'label'
            
            # ç‰©ç†æ•°æ®ï¼ˆå¯é€‰ï¼‰
            self.physical_data = self.data.get('physical_data', None)
        else:
            # æ—§æ¥å£ï¼šfeed_data(feats_A, feats_B, data)
            self.feats_A = args[0]
            self.feats_B = args[1]
            self.data = self.set_device(args[2])
            self.label = self.data.get('label', self.data.get('L'))
            self.physical_data = self.data.get('physical_data', None)
    
    def forward_cd(self, features_A, features_B):
        """å˜åŒ–æ£€æµ‹å‰å‘ä¼ æ’­"""
        change_map = self.netCD(features_A, features_B)
        return change_map
    
    # å…¼å®¹æ—§æ¥å£çš„optimize_parameters
    def optimize_parameters(self, *args, **kwargs):
        """ä¼˜åŒ–å‚æ•° - å…¼å®¹æ–°æ—§æ¥å£"""
        if len(args) >= 2:
            # æ–°æ¥å£ï¼šoptimize_parameters(features_A, features_B, current_epoch=None)
            features_A = args[0]
            features_B = args[1]
            current_epoch = kwargs.get('current_epoch', None)
        else:
            # æ—§æ¥å£ï¼šoptimize_parameters() - ä½¿ç”¨ä¿å­˜çš„ç‰¹å¾
            if hasattr(self, 'feats_A') and hasattr(self, 'feats_B'):
                features_A = self.feats_A
                features_B = self.feats_B
            elif hasattr(self, '_temp_features_A') and hasattr(self, '_temp_features_B'):
                features_A = self._temp_features_A
                features_B = self._temp_features_B
            else:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾è¿›è¡Œä¼˜åŒ–")
            current_epoch = kwargs.get('current_epoch', None)
        
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        self.pred_cm = self.forward_cd(features_A, features_B)
        self.change_prediction = self.pred_cm  # ä¸ºäº†å…¼å®¹æ€§
        
        # è®¡ç®—æŸå¤±
        if hasattr(self.criterion, 'forward') and 'physical_data' in self.criterion.forward.__code__.co_varnames:
            # ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°
            self.loss_cd = self.criterion(
                self.pred_cm, 
                self.label.long(),
                physical_data=self.physical_data,
                epoch=current_epoch
            )
        else:
            # æ ‡å‡†æŸå¤±å‡½æ•°
            self.loss_cd = self.criterion(self.pred_cm, self.label.long())
        
        # åå‘ä¼ æ’­
        self.loss_cd.backward()
        self.optimizer.step()
        
        # è®°å½•æŸå¤±
        self.log_dict['l_cd'] = self.loss_cd.item()
    
    # å…¼å®¹æ—§æ¥å£çš„test
    def test(self, *args):
        """æµ‹è¯•æ¨¡å¼ - å…¼å®¹æ–°æ—§æ¥å£"""
        self.netCD.eval()
        with torch.no_grad():
            if len(args) >= 2:
                # æ–°æ¥å£ï¼štest(features_A, features_B)
                features_A = args[0]
                features_B = args[1]
            else:
                # æ—§æ¥å£ï¼štest() - ä½¿ç”¨ä¿å­˜çš„ç‰¹å¾
                if hasattr(self, 'feats_A') and hasattr(self, 'feats_B'):
                    features_A = self.feats_A
                    features_B = self.feats_B
                elif hasattr(self, '_temp_features_A') and hasattr(self, '_temp_features_B'):
                    features_A = self._temp_features_A
                    features_B = self._temp_features_B
                else:
                    raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾è¿›è¡Œæµ‹è¯•")
            
            self.pred_cm = self.forward_cd(features_A, features_B)
            self.change_prediction = self.pred_cm
            
            # è®¡ç®—æŸå¤±
            # ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨self.criterionï¼Œå…¼å®¹self.loss_func
            if hasattr(self, 'criterion') and self.criterion is not None:
                l_cd = self.criterion(self.pred_cm, self.label.long())
            elif hasattr(self, 'loss_func') and self.loss_func is not None:
                l_cd = self.loss_func(self.pred_cm, self.label.long())
            self.log_dict['l_cd'] = l_cd.item()
        
        if self.is_train:
            self.netCD.train()
    
    def get_current_log(self):
        """è·å–å½“å‰æ—¥å¿—"""
        return self.log_dict
    
    def get_current_visuals(self):
        """è·å–å½“å‰å¯è§†åŒ–ç»“æœ"""
        out_dict = OrderedDict()
        
        # é¢„æµ‹ç»“æœ
        if hasattr(self, 'pred_cm'):
            pred_cm = torch.argmax(self.pred_cm, dim=1, keepdim=False)
            out_dict['pred_cm'] = pred_cm.detach().float().cpu()
        
        # çœŸå®æ ‡ç­¾
        if hasattr(self, 'label'):
            out_dict['gt_cm'] = self.label.detach().float().cpu()
        elif hasattr(self, 'data') and 'L' in self.data:
            out_dict['gt_cm'] = self.data['L'].detach().float().cpu()
        
        return out_dict
    
    def print_network(self):
        """æ‰“å°ç½‘ç»œç»“æ„"""
        s, n = self.get_network_description(self.netCD)
        if isinstance(self.netCD, nn.DataParallel):
            net_struc_str = f'{self.netCD.__class__.__name__} - {self.netCD.module.__class__.__name__}'
        else:
            net_struc_str = f'{self.netCD.__class__.__name__}'
        
        logger.info(f'Change Detection Network structure: {net_struc_str}, with parameters: {n:,d}')
        logger.info(s)
    
    def save_network(self, epoch, is_best_model=False):
        """ä¿å­˜ç½‘ç»œå‚æ•° - å…¼å®¹æ—§æ¥å£"""
        cd_gen_path = os.path.join(
            self.opt['path']['checkpoint'], f'cd_model_E{epoch}_gen.pth')
        cd_opt_path = os.path.join(
            self.opt['path']['checkpoint'], f'cd_model_E{epoch}_opt.pth')
        
        if is_best_model:
            best_cd_gen_path = os.path.join(
                self.opt['path']['checkpoint'], 'best_cd_model_gen.pth')
            best_cd_opt_path = os.path.join(
                self.opt['path']['checkpoint'], 'best_cd_model_opt.pth')

        # ä¿å­˜CDæ¨¡å‹å‚æ•°
        network = self.netCD
        if isinstance(self.netCD, nn.DataParallel):
            network = network.module
        state_dict = network.state_dict()
        for key, param in state_dict.items():
            state_dict[key] = param.cpu()
        torch.save(state_dict, cd_gen_path)
        if is_best_model:
            torch.save(state_dict, best_cd_gen_path)

        # ä¿å­˜CDä¼˜åŒ–å™¨å‚æ•°
        opt_state = {'epoch': epoch,
                     'scheduler': None, 
                     'optimizer': None}
        opt_state['optimizer'] = self.optimizer.state_dict()
        torch.save(opt_state, cd_opt_path)
        if is_best_model:
            torch.save(opt_state, best_cd_opt_path)

        # æ‰“å°ä¿¡æ¯
        logger.info(f'Saved current CD model in [{cd_gen_path}] ...')
        if is_best_model:
            logger.info(f'Saved best CD model in [{best_cd_gen_path}] ...')
    
    def load_network(self):
        """åŠ è½½é¢„è®­ç»ƒçš„CDç½‘ç»œ"""
        load_path = self.opt.get('path_cd', {}).get('resume_state')
        if load_path is not None:
            logger.info(f'Loading pretrained model for CD model [{load_path}] ...')
            gen_path = f'{load_path}_gen.pth'
            opt_path = f'{load_path}_opt.pth'
            
            # åŠ è½½å˜åŒ–æ£€æµ‹æ¨¡å‹
            if os.path.exists(gen_path):
                network = self.netCD
                if isinstance(self.netCD, nn.DataParallel):
                    network = network.module
                network.load_state_dict(torch.load(gen_path), strict=True)
                
                if self.is_train and os.path.exists(opt_path):
                    opt = torch.load(opt_path)
                    self.optimizer.load_state_dict(opt['optimizer'])
                    if 'epoch' in opt:
                        self.begin_epoch = opt['epoch']
                    if 'iter' in opt:
                        self.begin_step = opt['iter']
    
    # CDç‰¹æœ‰çš„æ–¹æ³•
    def _update_metric(self):
        """æ›´æ–°æŒ‡æ ‡"""
        G_pred = self.pred_cm.detach()
        G_pred = torch.argmax(G_pred, dim=1)
        
        current_score = self.running_metric.update_cm(
            pr=G_pred.cpu().numpy(), 
            gt=self.label.detach().cpu().numpy()
        )
        return current_score
    
    def _collect_running_batch_states(self):
        """æ”¶é›†å½“å‰æ‰¹æ¬¡çš„è¿è¡ŒçŠ¶æ€"""
        self.running_acc = self._update_metric()
        self.log_dict['running_acc'] = self.running_acc.item()
    
    def _collect_epoch_states(self):
        """æ”¶é›†epochçŠ¶æ€"""
        scores = self.running_metric.get_scores()
        self.epoch_acc = scores['mf1']
        self.log_dict['epoch_acc'] = self.epoch_acc.item()
        
        for k, v in scores.items():
            self.log_dict[k] = v
    
    def _clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.running_metric.clear()
    
    def _update_lr_schedulers(self):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if hasattr(self, 'exp_lr_scheduler_netCD') and self.exp_lr_scheduler_netCD:
            self.exp_lr_scheduler_netCD.step()
        elif hasattr(self, 'schedulers'):
            for scheduler in self.schedulers:
                if scheduler:
                    scheduler.step()
    
    def get_physics_constraint_details(self):
        """è·å–ç‰©ç†çº¦æŸæŸå¤±çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºåˆ†æï¼‰"""
        if hasattr(self.criterion, 'get_constraint_details'):
            return self.criterion.get_constraint_details(
                self.pred_cm if hasattr(self, 'pred_cm') else self.change_prediction, 
                self.label, 
                self.physical_data
            )
        return {}


# ä¸ºäº†å‘åå…¼å®¹
CD = DDPMCDModel


def create_CD_model(opt):
    """åˆ›å»ºå˜åŒ–æ£€æµ‹æ¨¡å‹"""
    model = DDPMCDModel(opt)
    logger.info(f'CD Model [{model.__class__.__name__}] is created.')
    return model